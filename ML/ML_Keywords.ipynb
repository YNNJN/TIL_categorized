{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝에서의 훈련 노하우에 대한 키워드를 정리함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **과대적합**이란 훈련 데이터의 반복으로 검증 세트에 대한 일반화 성능이 더 이상 높아지지 않고 감소되는 상태\n",
    "     \n",
    "     \n",
    "     \n",
    "- **과소적합**이란 훈련데이터의 손실이 낮아 테스트 데이터의 손실이 낮은 상태\n",
    "    - 간단한 모델이 복잡한 모델보다 과대적합 될 가능성이 적음\n",
    "    - 이 때 간단한 모델이란 파라미터 값 분포의 엔트로피가 작은 모델\n",
    "    - 그러므로 과대적합을 완화하기 위해 네트워크 복잡도에 제한을 두어 가중치가 작은 값을 가지도록 강제함 (Regularization)\n",
    "\n",
    "\n",
    "- **Capacity**\n",
    "    - 딥러닝에서 모델에 있는 학습 파라미터의 수를 의미\n",
    "    - 과적합을 막는 가장 단순한 방법이 모델의 크기, 즉 모델에 있는 학습 파라미터 수를 줄이는 것\n",
    "    - 파라미터의 수는 은닉 계층의 수와 각 층의 유닛 수에 의해 결정됨\n",
    "\n",
    "\n",
    "- **편차와 분산과 표준편차**\n",
    "    - 편차는 관측값과 평균의 차이\n",
    "    - 분산은 변량들이 퍼져있는 정도, 분산이 크면 불안정하다는 의미\n",
    "    - 표준편차는 분산은 수치가 크니, 제곱근으로 적당히 줄인 값\n",
    "    \n",
    "\n",
    "- **평균제곱오차(MSE, Mean Squarred Error)**\n",
    "    - 모든 샘플에서 실제 값과 모델 값 사이의 차이를 제곱한 값의 평균\n",
    "\n",
    "\n",
    "- **정형화(Regularization)**\n",
    "    - 모델의 복잡도에 패널티를 부여함. 과적합을 방지하는 데 도움\n",
    "    - 즉 복잡할 수 있는 모델의 유연성을 어느 정도 감소시켜주는 역할\n",
    "    - L1 정형화, L2 정형화, Drop out, Early stopping\n",
    "\n",
    "\n",
    "- **L1 정형화**\n",
    "    - 가중치의 절대값 합에 비례하여 가중치에 패널티를 주는 정규화 유형\n",
    "    - sparse feature에 의존하는 모델에서 L1 정규화는 관련성이 없거나 매우 낮은 특성의 가중치를 정확히 0으로 만들어 해당 피처를 모델이 무시하도록 함 (feature selection 효과)\n",
    "    - 참고로 L1 손실은 모델이 예측하는 값과 라벨의 실제 값 차이의 절대값에 기초한 손실 함수 (L2 손실보다 이상점에 둔감함)\n",
    "\n",
    "\n",
    "- **L2 정형화**\n",
    "    - 가중치의 제곱의 합에 비례하여 가중치에 패널티를 주는 정규화 유형\n",
    "    - 높은 긍정값 또는 낮은 부정값을 갖는 이상적 가중치를 0은 아니지만 0에 가깝게 유도하는 데 도움이 됨\n",
    "    - 선형 모델의 일반화를 항상 개선함\n",
    "    - 참고로 L2 손실은 라벨이 있는 인스턴스에 대한 모델의 예측값과 라벨의 실제 값 차이의 제곱을 계산함 (손실함수로서 선형 회귀에 사용됨)\n",
    "\n",
    "\n",
    "- **패널티 개념**\n",
    "    - 손실함수의 비용 텀에 패널티를 추가함\n",
    "    - 패널티 텀에 대한 가중치에 따라 패널티 항이 있는 손실 함수는 해의 norm이 크면 손실이 큰 것처럼 행세함\n",
    "    - 모델 복잡성에 패널티를 주는 방법을 정규화라고 함\n",
    "\n",
    "\n",
    "- **Sparse feature**\n",
    "    - 대부분의 값이 0이거나 비어있는 특성 벡터\n",
    "    - 예로 1값 하나와 0값 백만 개를 포함하는 벡터는 희소 벡터임\n",
    "    \n",
    "    \n",
    "- **Feature selection**\n",
    "    - 독립 변수 중에서 중복되거나 종속변수(y)와 관련 없는 변수들을 제거하여, y를 가장 잘 예측하는 변수들의 조합을 찾아내는 것 (최적화)\n",
    "    - 학습 시간 줄이고 모델의 분산을 줄임으로써 보다 robust하게 학습됨. 해석도 쉬워짐\n",
    "\n",
    "\n",
    "- **Drop out**\n",
    "    - 단일 경사 스텝이 일어날 때마다 특정 네트워크 레이어의 유닛을 고정된 개수만큼 무작위로 선택하여 삭제함\n",
    "    - 즉 네트워크 층에 드롭아웃을 적용하여 훈련하는 동안 무작위로 층의 일부 출력 특성을 제외시킴\n",
    "    - 드롭아웃하는 유닛이 많을수록 정규화가 강력한 것\n",
    "    - 네트워크를 학습시켜 더 작은 네트워크로 이루어진 대규모 앙상블(여러 모델의 예측을 병합한 결과)을 모방하도록 하는 방식과 유사\n",
    "    - 신경망을 학습시키는 데 유용한 정규화 형태임\n",
    "\n",
    "\n",
    "- **Data augumentation**\n",
    "    - 데이터를 늘리는 작업, 기존 데이터의 정보량을 보존한 상태로 노이즈를 줌 (내가 가진 정보량은 변하지 않음)\n",
    "    - 단지 정보량에 약간의 변화를 주는 것, 데이터의 강력하게 표현되는 고유의 특징을 느슨하게 만들어냄 -> 과대적합 막을 수 있고, 예측 범위를 약간 넓힐 수 있음\n",
    "        - 과대적합을 해결하기 위한 Regularization, Normalizaion -> 편향을 조금 죽이는 정도에 그침\n",
    "        - 학습의 방향성을 넓히는 방법을 고민 -> 적당한 힘으로 학습 면적을 아주 조금 골고루 넓히자\n",
    "        - 고유 정보가 학습될 때 해당 정보가 맵핑된 공간의 영역이 조금 넓어지면서 동시에 크게 벗어나지 않도록 학습하게 됨\n",
    "\n",
    "\n",
    "- **Xavier Initialization**\n",
    "    - w = np.random.randn(fan_in, fan_out)/np.sqrt(fan_in)\n",
    "    - 입력값(fan_in)과 출력값(fan_out) 사이 난수를 선택하여 입력값의 제곱근으로 나누는 것\n",
    "    - Xavier initializer를 통해 fan_in과 fan_out에 대한 fine tuning이 가능함\n",
    "    - fan_in과 fan_out은 미분과 적분의 관계인데\n",
    "        - 정확한 결과값 위해서는 전 단계에서 미분하거나 다음 단계에서 적분할 경우 서로의 값이 나와야\n",
    "        - 이 사이에서 정확한 값을 맞춰주는 변수를 찾는 것이 Xavier Initializer의 역할\n",
    "\n",
    "\n",
    "- **He initialization**\n",
    "    - w = np.random.randn(fan_in, fan_out)/np.sqrt(fan_in**/2**)\n",
    "    - 입력값을 반으로 나눈 제곱근을 사용함 (Xavier보다 넓은 범위의 난수를 생성함)\n",
    "\n",
    "\n",
    "- **Learning Rate**\n",
    "    - 경사하강법을 통해 모델을 학습시키는 데 사용되는 스칼라 값\n",
    "    - 각 반복에서 경사하강법 알고리즘은 학습률을 경사에 곱함\n",
    "        - 이 곱셈의 결과를 경사 스텝이라고 함\n",
    "    - 핵심적 하이퍼파라미터임\n",
    "\n",
    "\n",
    "- **Normalization**\n",
    "    - 실제 값 범위를 표준값 범위(-1 ~ +1 또는 0~1)로 변환하는 과정\n",
    "\n",
    "\n",
    "- **Batch norm**\n",
    "    - 가중치의 초기값을 적절히 설정하면 각 층의 활성화 값 분포가 적당히 퍼지면서 학습이 원활하게 수행되는데, 이 점에 착안하여 각 층의 활성화 값을 강제로 퍼뜨리는 것을 말함\n",
    "    - 일반적으로 신경망 모델에 데이터를 입력하기 전 전체 데이터 세트를 정규화하지만, 배치 정규화는 미니 배치 별로 학습 바로 전에 데이터를 정규화하는 방식임\n",
    "    - 학습 속도를 빠르게 하고, 매개변수의 초기값에 크게 의존하지 않으며, 과대적합을 억제함\n",
    "    - 기본적으로 데이터 분포가 평균이 0, 분산이 1이 되도록 정규화 함\n",
    "\n",
    "\n",
    "- **SGD / AdaGrad / Adam**\n",
    "    - **GD** \n",
    "        - 학습데이터의 조건에 따라 모델의 매개변수를 기준으로 손실의 경사를 계산하여 손실을 최소화하는 기법\n",
    "        - 즉 매개변수를 반복적으로 조정하면서 손실을 최소화하는 가중치와 편향의 가장 적절한 조합을 점진적으로 찾음\n",
    "    - **SGD(Stochastic Gradient Descent)**\n",
    "        - 배치 크기가 1인 경사하강법 알고리즘\n",
    "        - 데이터 세트에서 무작위로 균일하게 선택한 하나의 인스턴스에 의존하여 각 단계의 예측 경사를 계산함\n",
    "\n",
    "    - **AdaGrad**\n",
    "        - 각 매개변수의 경사를 재조정하여 사실상 각 매개변수에 독립적인 학습률을 부여하는 정교한 경사하강법 알고리즘\n",
    "\n",
    "\n",
    "- **커널 서포트 벡터 머신(KSVM, Kernel Support Vector Machine)**\n",
    "    - 입력 데이터 벡터를 상위 차원 공간에 매핑하여 포지티브와 네거티브 클래스 사이의 간격을 최대화하는 것을 목표로 하는 분류 알고리즘\n",
    "    - 힌지 손실이라는 손실 함수 사용 (loss = 'hinge')\n",
    "\n",
    "\n",
    "- **pipeline**\n",
    "    - 머신러닝 알고리즘의 기반이 되는 인프라\n",
    "    - 파이프라인에는 데이터 수집, 학습 데이터 파일에 데이터 넣기, 하나 이상의 모델 학습, 프로덕션 환경으로 내보내기 등이 포함됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7701_염라대왕의 이름 정렬\n",
    "#이름 길이가 짧을수록, 같은 길이면 사전 순으로 앞에, 같은 이름은 하나만\n",
    "for tc in range(int(input())):\n",
    "    n = int(input())\n",
    "    names = set()\n",
    "    for i in range(n):\n",
    "        names.add(input())\n",
    "    names = sorted(names) # names.sort(key = lambda x: (len(x), x))\n",
    "    names.sort(key = len)\n",
    "    print('#{}'.format(tc+1))\n",
    "    for name in names:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7465_창용마을 무리의 개수\n",
    "def dfs(v):\n",
    "    visited[v] = 1\n",
    "    for e in adj[v]:\n",
    "        if visited[e]: continue #continue를 추가한 것만으로 실행시간 80% 정도로 줄일 수 있음\n",
    "        dfs(e)\n",
    "\n",
    "for tc in range(int(input())):\n",
    "    n,m = map(int, input().split())\n",
    "    adj = [[] for _ in range(n+1)]\n",
    "    visited = [0] * (n + 1)\n",
    "    cnt = 0\n",
    "\n",
    "    for _ in range(m):\n",
    "        u,v = list(map(int, input().split()))\n",
    "        adj[u].append(v)\n",
    "        adj[v].append(u)\n",
    "\n",
    "    for i in range(1, len(visited)):\n",
    "        if not visited[i]:\n",
    "            cnt += 1\n",
    "            dfs(i)\n",
    "\n",
    "    print('#{} {}'.format(tc+1, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6959_이상한 나라의 덧셈게임\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
