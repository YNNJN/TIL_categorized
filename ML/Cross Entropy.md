### 정보이론과 엔트로피

- **정보이론 (Information Theory)**
  - 정보와 수량화 -> 계산 가능 -> 컴퓨터에게 작업시킴
  - 어느 쪽이 더 많은 정보를 전달하고 있는가? 정보량이 많은 쪽?
    - `고비사막에 눈이 왔다` vs `대관령에 눈이 왔다`
      - 고비사막! 더 드문 확률이기 때문
  - 메시지의 정보량을 확률로 측정함
  - 확률이 낮을수록 정보량은 많아짐 (확률과 정보량은 반비례 관계)
  - 부등식의 학문, 어떤 상황에서 부등식이 등식이 되는가?

- **엔트로피**
  - 확률변수 `x`의 불확실성 또는 무질서도를 측정
  - 무질서하고 불확실성이 커질수록 엔트로피가 높아짐
    - n개의 사건(정의역)이 있을 때, 각 사건이 동일한 확률을 가질 때 엔트로피가 가장 큼
    - 정의역의 크기가 크면 엔트로피도 커짐
      - 동전의 엔트로피가 주사위의 엔트로피보다 작음
    - 주사위를 굴려 1이 나올 확률은 엔트로피가 어마어마하게 높음
    - 엔트로피가 높다 = 트레인이 어렵다 = 손실함수 값이 크다
- **교차 엔트로피 (Cross Entropy)**
  - 두 확률분포 간의 엔트로피를 측정해야 할 때 사용
  - 머신러닝을 통한 예측 모형에서 훈련 데이터는 실제 분포인 P를 알 수 있기 때문에 교차 엔트로피 계산 가능
  - P와 Q 사이의 교차 엔트로피
    - P의 엔트로피 + P와 Q 간의 KL Divergence
    - 두번째 텀은 Q를 P로 바꾸는 데 드는 비용을 의미
  - KL Divergence
    - 쿨백 - 라이블러 발산(Kullback-Leibler Divergence, KL)
    - 가짜분포 q가 진분포 p를 얼마나 잘 따라했는가
      - q(x)가 p(x)보다 클 때, 즉 넉넉하게 예상했을때는 패널티를 깎아줌
      - 반면 q(x)가 p(x)보다 작은 상황, 즉 부족하게 예상을 하면 패널티가 발생
    - 정보량의 차이, 정보 엔트로피 차이, 확률분포 간의 차이를 계산하는 함수
    - 크로스 엔트로피에서 두 엔트로피의 차이로 계산됨
    - `두 확률분포가 얼마나 다른가?`
  - 엔트로피가 0인 상황에서(진분포가 정답이 하나인 상황) 
    - KL Divergence = CE(P,Q)
    - CE(Xa, Ya) = -logP(Ya=1)
    - "정답 클래스의 확률값만 두고 나머진 버린 후, 해당 확률값은 로그를 씌워서 음수로 만든다"
    - 즉 log-likelihood
  - Shannon entropy
    - I(x) = -logP(x)
    - 사건의 분포가 결정적(deterministic)이라면 해당 확률분포의 불확실성 정도를 나타내는 엔트로피는 낮아짐
    - 분포가 균등적(uniform)일 수록 엔트로피는 높아짐
  - MSE vs CE
    - MSE를 사용하면 가중치 계산에서 기울기 값에 (output) * (1 - output)이라는 조정 요소가 포함됨
    - 계산 된 출력이 0.0 또는 1.0에 가깝거나 가까워짐에 따라 (output) * (1 - output)의 값은 점점 작아짐
    - 조정 요소가 점점 작아지면서 가중치 변화도 점점 작아지고 학습 진행이 멈출 수 있음 (slowdown)
    - CE의 경우 (output) * (1 - output) 항이 사라져 가중치 변화가 점점 작아지거나 하지 않음